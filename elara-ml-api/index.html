
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Elara Hub</title>
    
    <link rel="stylesheet" href="https://elaraproject.github.io/elara-hub/index.css">
    <!--KaTeX-->
    <link rel="stylesheet" href="https://elaraproject.github.io/elara-hub/katex/katex.min.css">

    <!--Open Sans-->
    <link rel="stylesheet" href="https://elaraproject.github.io/elara-hub/open-sans/open-sans.css">
    
</head>
<body>
    <nav>
    	<span class="header active">
    		<a href="https://elaraproject.github.io/elara-hub">Elara Hub</a> :: <a href="https://elaraproject.github.io/elara-hub/elara-hub-index/">Home</a>
    	</span>
    	<a class="header mobile-only" href="https://elaraproject.github.io/elara-hub/menu/">Menu</a>
    	<ul>
    	 
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/6-month-roadmap-1/">6-month roadmap 1</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/all-about-pdes/">All about partial differential equations</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/alt-de-solver/">Alternate differential equation solver</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/black-hole-raytracing/">Black hole raytracing</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/elara-gfx-continuing-work/">Elara GFX continuing work plan</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/elara-hub-creation/">Elara Hub creation plan</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/elara-hub-index/">Elara Hub Index</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/meta/">Elara Hub Meta</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/markdown-guide/">Elara markdown guide</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/elara-math-optimization/">Elara math optimization strategies</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/elara-ml-api/">Elara ML API proposal</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/elara-array-api-plan/">Elara-array API</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/extra/">Extra content</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/focusing-chamber-1/">Focusing chamber theoretical analysis 1</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/geometrized/">Geometrized units</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/h2-co2/">Hydrogen from CO2 production</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/integration-techniques/">Integration techniques</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/menu/">Menu (for mobile)</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/multivar/">Multivariable calculus guide</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/stages/">Project development stages</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/app-plans/">Project Elara app plans</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/library-plans/">Project Elara library plans</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/outreach-plan/">Project Elara Outreach Plan</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/funny/">Project humour and jokes</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/project-philosophy/">Project Philosophy</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/eval-christoffels/">Quickly evaluating the Christoffel symbols</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/relativity-hub/">Relativity Hub</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/future-extensions/">Speculative future extensions</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/standard-notation/">Standardized calculus notation</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/handbook-addthings/">Things to add to Elara Handbook</a></li>
    	 
    	 	<li><a href="https://elaraproject.github.io/elara-hub/visualization-outreach/">Visualization-based outreach</a></li>
    	 
    	</ul>
    </nav>

    <main>
    	<article>
	    
	<h1>Elara ML API proposal</h1>
	<!--eventually add in authors-->
	<p><code>elara-ml</code> API - so there are actually 3 APIs currently being compared. The first one is PyTorch-style:</p>
<p><code>elara-ml</code> API - so there are actually 3 APIs currently being compared. The first one is PyTorch-style:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span style="color:#b48ead;">pub struct </span><span>MyModel {
</span><span>    </span><span style="color:#bf616a;">input_layer</span><span>: Input,
</span><span>    </span><span style="color:#bf616a;">hidden_1</span><span>: Dense,
</span><span>    </span><span style="color:#bf616a;">hidden_2</span><span>: Dense,
</span><span>    </span><span style="color:#bf616a;">output_layer</span><span>: Output
</span><span>}
</span><span>
</span><span style="color:#b48ead;">impl </span><span>MyModel {
</span><span>    </span><span style="color:#65737e;">// x and y are here only for shape determination
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">new</span><span>(</span><span style="color:#bf616a;">x</span><span>: Tensor, </span><span style="color:#bf616a;">y</span><span>: Tensor) -&gt; MyModel {
</span><span>        </span><span style="color:#65737e;">// automatic shape determination by passing
</span><span>        </span><span style="color:#65737e;">// another layer as first argument
</span><span>        </span><span style="color:#b48ead;">let</span><span> input_layer = Input::new(x);
</span><span>        </span><span style="color:#b48ead;">let</span><span> hidden_1 = Dense::new(input_layer, </span><span style="color:#d08770;">16</span><span>);
</span><span>        </span><span style="color:#b48ead;">let</span><span> hidden_2 = Dense::new(hidden_1, </span><span style="color:#d08770;">16</span><span>);
</span><span>        </span><span style="color:#b48ead;">let</span><span> output_layer = Output::new(hidden_2, y);
</span><span>        
</span><span>        MyModel {
</span><span>            input_layer,
</span><span>            hidden_1,
</span><span>            hidden_2,
</span><span>            output_layer
</span><span>        }
</span><span>    }
</span><span>}
</span><span>
</span><span style="color:#b48ead;">impl </span><span>Model </span><span style="color:#b48ead;">for </span><span>MyModel {
</span><span>    </span><span style="color:#65737e;">// Models can only have one output, for
</span><span>    </span><span style="color:#65737e;">// multi-input-output neural networks you
</span><span>    </span><span style="color:#65737e;">// need to chain together multiple Models
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">forward</span><span>(</span><span style="color:#bf616a;">x</span><span>: Tensor) -&gt; Tensor {
</span><span>        </span><span style="color:#65737e;">// These absolutely don&#39;t need to
</span><span>        </span><span style="color:#65737e;">// be in the same order as you declared
</span><span>        </span><span style="color:#65737e;">// in new() (but probably should be so
</span><span>        </span><span style="color:#65737e;">// that the auto shape determination works)
</span><span>        </span><span style="color:#b48ead;">let</span><span> a = </span><span style="color:#bf616a;">self</span><span>.input_layer.</span><span style="color:#96b5b4;">forward</span><span>(x);
</span><span>        </span><span style="color:#b48ead;">let</span><span> b = </span><span style="color:#bf616a;">self</span><span>.hidden_1.</span><span style="color:#96b5b4;">forward</span><span>(a);
</span><span>        </span><span style="color:#b48ead;">let</span><span> c = </span><span style="color:#bf616a;">self</span><span>.hidden_2.</span><span style="color:#96b5b4;">forward</span><span>(b);
</span><span>        </span><span style="color:#b48ead;">let</span><span> d = </span><span style="color:#bf616a;">self</span><span>.output_layer.</span><span style="color:#96b5b4;">forward</span><span>(c);
</span><span>        d
</span><span>    }
</span><span>}
</span><span>
</span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">main</span><span>() {
</span><span>    </span><span style="color:#b48ead;">let</span><span> model = MyModel::new();
</span><span>    model.</span><span style="color:#96b5b4;">compile</span><span>(Optimizers::</span><span style="color:#d08770;">SGD</span><span>);
</span><span>    model.</span><span style="color:#96b5b4;">fit</span><span>(&amp;x, &amp;y, </span><span style="color:#d08770;">500</span><span>, </span><span style="color:#d08770;">0.00001</span><span>, </span><span style="color:#d08770;">true</span><span>);
</span><span>}
</span></code></pre>
<p>This API makes it easiest to use pre-made models, because you can simply import the model and compile it. However, it might be too much abstraction - it can be a little hard to see what the model is actually doing, especially with methods like <code>compile()</code> and <code>fit()</code> that no longer have a 1-1 correspondence with performing operations on tensors.</p>
<p>The second uses a macro <code>Sequential!</code> to imitate Keras's sequential API. This makes it easiest to learn, but again, abstracts away too much, which is not ideal, especially given how much debugging is done when making NNs.</p>
<p>The third is most barebones, and is the Jax-inspired API. It looks like this:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span style="color:#65737e;">// This is just a convenient way of
</span><span style="color:#65737e;">// holding layers, there is nothing
</span><span style="color:#65737e;">// special about this struct
</span><span style="color:#b48ead;">struct </span><span>Layers {
</span><span>    </span><span style="color:#b48ead;">pub </span><span style="color:#bf616a;">input_layer</span><span>: Input,
</span><span>    </span><span style="color:#b48ead;">pub </span><span style="color:#bf616a;">hidden_1</span><span>: Dense,
</span><span>    </span><span style="color:#b48ead;">pub </span><span style="color:#bf616a;">hidden_2</span><span>: Dense,
</span><span>    </span><span style="color:#b48ead;">pub </span><span style="color:#bf616a;">output_layer</span><span>: Output
</span><span>}
</span><span>
</span><span style="color:#b48ead;">impl </span><span>Layers {
</span><span>    </span><span style="color:#65737e;">// x and y are here only for shape determination
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">new</span><span>(</span><span style="color:#bf616a;">x</span><span>: Tensor, </span><span style="color:#bf616a;">y</span><span>: Tensor) -&gt; Layers {
</span><span>        </span><span style="color:#65737e;">// automatic shape determination by passing
</span><span>        </span><span style="color:#65737e;">// another layer as first argument
</span><span>        </span><span style="color:#b48ead;">let</span><span> input_layer = Input::new(x);
</span><span>        </span><span style="color:#b48ead;">let</span><span> hidden_1 = Dense::new(input_layer, </span><span style="color:#d08770;">16</span><span>);
</span><span>        </span><span style="color:#b48ead;">let</span><span> hidden_2 = Dense::new(hidden_1, </span><span style="color:#d08770;">16</span><span>);
</span><span>        </span><span style="color:#b48ead;">let</span><span> output_layer = Output::new(hidden_2, y);
</span><span>        
</span><span>        MyModel {
</span><span>            input_layer,
</span><span>            hidden_1,
</span><span>            hidden_2,
</span><span>            output_layer
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span style="color:#65737e;">// Note: for zero_grad()
</span><span>    </span><span style="color:#65737e;">// and update(), these can be
</span><span>    </span><span style="color:#65737e;">// made less verbose by creating an
</span><span>    </span><span style="color:#65737e;">// iter() method - see
</span><span>    </span><span style="color:#65737e;">// https://stackoverflow.com/questions/30218886/how-to-implement-iterator-and-intoiterator-for-a-simple-struct
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">zero_grad</span><span>(&amp;</span><span style="color:#bf616a;">self</span><span>) {
</span><span>        </span><span style="color:#bf616a;">self</span><span>.input_layer.</span><span style="color:#96b5b4;">zero_grad</span><span>();
</span><span>        </span><span style="color:#bf616a;">self</span><span>.hidden_1.</span><span style="color:#96b5b4;">zero_grad</span><span>();
</span><span>        </span><span style="color:#bf616a;">self</span><span>.hidden_2.</span><span style="color:#96b5b4;">zero_grad</span><span>();
</span><span>        </span><span style="color:#bf616a;">self</span><span>.output_layer.</span><span style="color:#96b5b4;">zero_grad</span><span>();
</span><span>    }
</span><span>
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">update</span><span>(&amp;</span><span style="color:#bf616a;">self</span><span>, </span><span style="color:#bf616a;">lr</span><span>: </span><span style="color:#b48ead;">f64</span><span>) {
</span><span>        </span><span style="color:#bf616a;">self</span><span>.input_layer.</span><span style="color:#96b5b4;">update</span><span>(lr);
</span><span>        </span><span style="color:#bf616a;">self</span><span>.hidden_1.</span><span style="color:#96b5b4;">update</span><span>(lr);
</span><span>        </span><span style="color:#bf616a;">self</span><span>.hidden_2.</span><span style="color:#96b5b4;">update</span><span>(lr);
</span><span>        </span><span style="color:#bf616a;">self</span><span>.output_layer.</span><span style="color:#96b5b4;">update</span><span>(lr);
</span><span>    }
</span><span>
</span><span>    </span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">save</span><span>(&amp;</span><span style="color:#bf616a;">self</span><span>) {
</span><span>        </span><span style="color:#b48ead;">let</span><span> weights = NNSerializer::new(&quot;</span><span style="color:#a3be8c;">weights.bin</span><span>&quot;);
</span><span>        </span><span style="color:#65737e;">// Add labels to weights; they will be referred
</span><span>        </span><span style="color:#65737e;">// to by their labels when the weights are loaded
</span><span>        weights.</span><span style="color:#96b5b4;">add</span><span>(</span><span style="color:#bf616a;">self</span><span>.input_layer, &quot;</span><span style="color:#a3be8c;">input_layer</span><span>&quot;);
</span><span>        weights.</span><span style="color:#96b5b4;">add</span><span>(</span><span style="color:#bf616a;">self</span><span>.hidden_1, &quot;</span><span style="color:#a3be8c;">hidden_1</span><span>&quot;);
</span><span>        weights.</span><span style="color:#96b5b4;">add</span><span>(</span><span style="color:#bf616a;">self</span><span>.hidden_2, &quot;</span><span style="color:#a3be8c;">hidden_2</span><span>&quot;);
</span><span>        weights.</span><span style="color:#96b5b4;">add</span><span>(</span><span style="color:#bf616a;">self</span><span>.output_layer, &quot;</span><span style="color:#a3be8c;">output_layer</span><span>&quot;);
</span><span>        weights.</span><span style="color:#96b5b4;">write</span><span>();
</span><span>    }
</span><span>}
</span><span>
</span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">forward</span><span>(</span><span style="color:#bf616a;">layers</span><span>: Layers, </span><span style="color:#bf616a;">x</span><span>: Tensor) -&gt; Tensor {
</span><span>    </span><span style="color:#b48ead;">let</span><span> a = layers.input_layer.</span><span style="color:#96b5b4;">forward</span><span>(x);
</span><span>    </span><span style="color:#b48ead;">let</span><span> b = layers.hidden_1.</span><span style="color:#96b5b4;">forward</span><span>(a);
</span><span>    </span><span style="color:#b48ead;">let</span><span> c = layers.hidden_2.</span><span style="color:#96b5b4;">forward</span><span>(b);
</span><span>    </span><span style="color:#b48ead;">let</span><span> d = layers.output_layer.</span><span style="color:#96b5b4;">forward</span><span>(c);
</span><span>    d
</span><span>}
</span><span>
</span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">mean_squared_error</span><span>(</span><span style="color:#bf616a;">y</span><span>: Tensor, </span><span style="color:#bf616a;">y_pred</span><span>: Tensor) -&gt; Tensor {
</span><span>    (&amp;y_pred - &amp;y).</span><span style="color:#96b5b4;">pow</span><span>(</span><span style="color:#d08770;">2</span><span>)
</span><span>}
</span><span>
</span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">main</span><span>() {
</span><span>    </span><span style="color:#65737e;">// load x and y...
</span><span>    </span><span style="color:#b48ead;">let</span><span> layers = Layers::new();
</span><span>    </span><span style="color:#b48ead;">let</span><span> pbar = TrainingProgress::new(); </span><span style="color:#65737e;">// used to display progress bars
</span><span>
</span><span>    </span><span style="color:#65737e;">// here we write our custom optimizer
</span><span>    </span><span style="color:#b48ead;">for</span><span> i in </span><span style="color:#d08770;">0</span><span>..</span><span style="color:#d08770;">1000 </span><span>{
</span><span>        </span><span style="color:#b48ead;">let</span><span> preds = </span><span style="color:#96b5b4;">forward</span><span>(layers, x);
</span><span>        </span><span style="color:#65737e;">// preds and loss are both tensors, so they
</span><span>        </span><span style="color:#65737e;">// can work with all the standard tensor methods,
</span><span>        </span><span style="color:#65737e;">// including output to graphviz files!
</span><span>        </span><span style="color:#b48ead;">let</span><span> loss = </span><span style="color:#96b5b4;">mean_squared_error</span><span>(y, preds);
</span><span>        pbar.</span><span style="color:#96b5b4;">update</span><span>(i, &amp;loss); </span><span style="color:#65737e;">// shows latest progress
</span><span>        </span><span style="color:#b48ead;">let</span><span> lr = </span><span style="color:#d08770;">1.0 </span><span>- </span><span style="color:#d08770;">0.9</span><span>*i/</span><span style="color:#d08770;">100.0
</span><span>        loss.</span><span style="color:#96b5b4;">backward</span><span>();
</span><span>        layers.</span><span style="color:#96b5b4;">update</span><span>(lr);
</span><span>        layers.</span><span style="color:#96b5b4;">zero_grad</span><span>();
</span><span>    }
</span><span>}
</span></code></pre>
<p>This approach has just the right amount of abstraction, and is very flexible, because it allows defining custom forward passes (with the ability to do multiple inputs or multiple outputs), custom loss functions, and custom optimizers. Furthermore, this API can easily interoperate with the PyTorch-style API. So this will be the API that is primarily focused on.</p>


	    </article>
    </main>

    <script>
    // Necessary until pulldown-cmark (which Zola depends on)
    // auto-appends a .tasklist class to all list items
    // that contain task lists
    document.querySelectorAll("input[type='checkbox']").forEach((el) => el.parentElement.classList.add("tasklist"));
    </script>

    <script defer src="https://elaraproject.github.io/elara-hub/katex/katex.min.js"></script>
    <script defer src="https://elaraproject.github.io/elara-hub/katex/contrib/auto-render.min.js"></script>

    <script>
    	// Loading KaTeX
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
              // customised options
              // • auto-render specific keys, e.g.:
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false},
                  {left: '\\[', right: '\\]', display: true}
              ],
              // • rendering keys, e.g.:
              throwOnError : false
            });
        });
    </script>
</body>
</html>
